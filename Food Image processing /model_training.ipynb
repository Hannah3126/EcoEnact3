{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import os  # Import the os module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.DS_Store', 'apple_pie', 'baby_back_ribs', 'baklava', 'beef_carpaccio', 'beef_tartare', 'beet_salad', 'beignets', 'bibimbap', 'bread_pudding', 'breakfast_burrito', 'bruschetta', 'caesar_salad', 'cannoli', 'caprese_salad', 'carrot_cake', 'ceviche', 'cheesecake', 'cheese_plate', 'chicken_curry', 'chicken_quesadilla', 'chicken_wings', 'chocolate_cake', 'chocolate_mousse', 'churros', 'clam_chowder', 'club_sandwich', 'crab_cakes', 'creme_brulee', 'croque_madame', 'cup_cakes', 'deviled_eggs', 'donuts', 'dumplings', 'edamame', 'eggs_benedict', 'escargots', 'falafel', 'filet_mignon', 'fish_and_chips', 'foie_gras', 'french_fries', 'french_onion_soup', 'french_toast', 'fried_calamari', 'fried_rice', 'frozen_yogurt', 'garlic_bread', 'gnocchi', 'greek_salad', 'grilled_cheese_sandwich', 'grilled_salmon', 'guacamole', 'gyoza', 'hamburger', 'hot_and_sour_soup', 'hot_dog', 'huevos_rancheros', 'hummus', 'ice_cream', 'lasagna', 'lobster_bisque', 'lobster_roll_sandwich', 'macaroni_and_cheese', 'macarons', 'miso_soup', 'mussels', 'nachos', 'omelette', 'onion_rings', 'oysters', 'pad_thai', 'paella', 'pancakes', 'panna_cotta', 'peking_duck', 'pho', 'pizza', 'pork_chop', 'poutine', 'prime_rib', 'pulled_pork_sandwich', 'ramen', 'ravioli', 'red_velvet_cake', 'risotto', 'samosa', 'sashimi', 'scallops', 'seaweed_salad', 'shrimp_and_grits', 'spaghetti_bolognese', 'spaghetti_carbonara', 'spring_rolls', 'steak', 'strawberry_shortcake', 'sushi', 'tacos', 'takoyaki', 'tiramisu', 'tuna_tartare', 'waffles']\n"
     ]
    }
   ],
   "source": [
    "# Set data directory to the path where you saved your dataset\n",
    "data_dir = r'D:\\Food Image processing\\food-101\\images'  # Use raw string or double backslashes\n",
    "\n",
    "# Verify the directory contents\n",
    "print(os.listdir(data_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 80800 images belonging to 101 classes.\n",
      "Found 20200 images belonging to 101 classes.\n"
     ]
    }
   ],
   "source": [
    "# Data preprocessing with increased augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set data directory\n",
    "data_dir = r'D:\\Food Image processing\\food-101\\images'\n",
    "\n",
    "# List of all classes (food categories)\n",
    "classes = [d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))]\n",
    "\n",
    "# Count the number of images per class\n",
    "class_counts = {cls: len(os.listdir(os.path.join(data_dir, cls))) for cls in classes}\n",
    "\n",
    "# Sort classes by number of images\n",
    "sorted_class_counts = dict(sorted(class_counts.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "# Display the top N classes as text\n",
    "top_n = 20\n",
    "for idx, (cls, count) in enumerate(sorted_class_counts.items()):\n",
    "    if idx < top_n:\n",
    "        print(f\"{idx + 1}. {cls}: {count} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "missing_files = []\n",
    "corrupted_files = []\n",
    "\n",
    "for cls in classes:\n",
    "    class_dir = os.path.join(data_dir, cls)\n",
    "    for filename in os.listdir(class_dir):\n",
    "        file_path = os.path.join(class_dir, filename)\n",
    "        try:\n",
    "            # Open the image file\n",
    "            img = Image.open(file_path)\n",
    "            img.verify()  # Verify that it is a readable image\n",
    "        except (IOError, SyntaxError) as e:\n",
    "            print(f'Corrupted file: {file_path}')\n",
    "            corrupted_files.append(file_path)\n",
    "        except FileNotFoundError:\n",
    "            print(f'Missing file: {file_path}')\n",
    "            missing_files.append(file_path)\n",
    "\n",
    "print(f\"Total missing files: {len(missing_files)}\")\n",
    "print(f\"Total corrupted files: {len(corrupted_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def show_random_images(data_dir, classes, num_classes=5, num_images=2):\n",
    "    # Randomly select a subset of classes\n",
    "    random_classes = random.sample(classes, min(num_classes, len(classes)))\n",
    "    \n",
    "    fig, axes = plt.subplots(num_images, len(random_classes), figsize=(15, num_images * 3))\n",
    "    for i, cls in enumerate(random_classes):\n",
    "        class_dir = os.path.join(data_dir, cls)\n",
    "        image_files = os.listdir(class_dir)\n",
    "        random_images = random.sample(image_files, min(num_images, len(image_files)))\n",
    "        for j, image_file in enumerate(random_images):\n",
    "            image_path = os.path.join(class_dir, image_file)\n",
    "            img = Image.open(image_path)\n",
    "            axes[j, i].imshow(img)\n",
    "            axes[j, i].axis('off')\n",
    "            if j == 0:\n",
    "                axes[j, i].set_title(cls, fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "show_random_images(data_dir, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m87910968/87910968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Define the model using InceptionV3 as base\n",
    "base_model = InceptionV3(input_shape=(150, 150, 3), include_top=False, weights='imagenet')\n",
    "base_model.trainable = False  # Freeze the base model\n",
    "\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dropout(0.5),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(101, activation='softmax')  # Adjust the number of classes\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    'best_model.keras',\n",
    "    save_best_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // validation_generator.batch_size,\n",
    "    epochs=50,\n",
    "    callbacks=[checkpoint_callback, early_stopping]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10 (v3.10.10:aad5f6a891, Feb  7 2023, 08:47:40) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "1605466cae8d9e7565f538ed703fa0177b3642d73334d4601e0ba658bb054d26"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
